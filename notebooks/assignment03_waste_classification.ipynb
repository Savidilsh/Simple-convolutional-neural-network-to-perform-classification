{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55ee5315",
   "metadata": {},
   "source": [
    "# EN3150 Assignment 03 - Waste Classification with CNNs\n",
    "\n",
    "This notebook follows the assignment brief to build, evaluate, and compare convolutional neural networks for image classification. It focuses on the supplied **RealWaste** dataset (substituting for the assignment's UCI requirement) and is structured so each numbered task in the PDF maps to clearly labelled sections below.\n",
    "\n",
    "> NOTE: Execute the notebook sequentially. Some cells cache intermediate artefacts (dataset splits, statistics, trained weights) so repeated runs stay reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dbf067",
   "metadata": {},
   "source": [
    "## Assignment Checklist\n",
    "\n",
    "The following sections mirror the original instructions:\n",
    "\n",
    "1. Environment setup & dataset selection (RealWaste).\n",
    "2. Dataset preparation and 70/15/15 split.\n",
    "3. CNN construction with configurable hyperparameters.\n",
    "4. Parameter justification (activations, kernel sizes, fully connected layers, dropout).\n",
    "5. Training for 20 epochs and tracking losses.\n",
    "6. Optimiser choice, learning-rate selection, and comparison with SGD variants.\n",
    "7. Evaluation with accuracy, confusion matrix, precision, and recall.\n",
    "8. Transfer learning with two pre-trained models (ResNet-18 and DenseNet-121), fine-tuning, and comparison.\n",
    "9. Discussion templates for interpreting the results (momentum impact, custom vs. pre-trained trade-offs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63be727",
   "metadata": {},
   "source": [
    "## 1. Environment & Dependencies\n",
    "\n",
    "Check library versions, confirm CUDA availability, and seed all relevant random number generators for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af93dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, models\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"PyTorch {torch.__version__} | Torchvision {torchvision.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a772501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "GLOBAL_SEED = 42\n",
    "set_seed(GLOBAL_SEED)\n",
    "GLOBAL_SEED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecce094",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation\n",
    "\n",
    "The assignment requests a 70/15/15 split. The helper below scans the `Dataset/RealWaste` folder, builds (or reuses) a reproducible split manifest, and reports class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dcaf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / \"Dataset\").exists() and (PROJECT_ROOT.parent / \"Dataset\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "DATA_ROOT = PROJECT_ROOT / \"Dataset/RealWaste\"\n",
    "SPLIT_MANIFEST = PROJECT_ROOT / \"Dataset/realwaste_splits.json\"\n",
    "STATS_CACHE = PROJECT_ROOT / \"Dataset/realwaste_stats.json\"\n",
    "ARTIFACT_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "ARTIFACT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "assert DATA_ROOT.exists(), f\"Expected dataset at {DATA_ROOT}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26afdab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_split_manifest(root: Path, manifest_path: Path, seed: int = 42) -> Dict[str, List[str]]:\n",
    "    if manifest_path.exists():\n",
    "        with manifest_path.open() as f:\n",
    "            payload = json.load(f)\n",
    "        print(f\"Loaded cached split manifest from {manifest_path}\")\n",
    "        return payload\n",
    "\n",
    "    classes = sorted([d for d in root.iterdir() if d.is_dir()])\n",
    "    samples: List[Tuple[str, str]] = []\n",
    "    for cls_dir in classes:\n",
    "        for img_path in sorted(cls_dir.glob(\"*.jpg\")):\n",
    "            rel_path = img_path.relative_to(root).as_posix()\n",
    "            samples.append((rel_path, cls_dir.name))\n",
    "\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(samples)\n",
    "\n",
    "    total = len(samples)\n",
    "    train_cut = int(total * 0.7)\n",
    "    val_cut = int(total * 0.85)\n",
    "\n",
    "    split_payload = {\n",
    "        \"seed\": seed,\n",
    "        \"root\": root.as_posix(),\n",
    "        \"class_names\": [cls.name for cls in classes],\n",
    "        \"splits\": {\n",
    "            \"train\": [path for path, _ in samples[:train_cut]],\n",
    "            \"val\": [path for path, _ in samples[train_cut:val_cut]],\n",
    "            \"test\": [path for path, _ in samples[val_cut:]],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    with manifest_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(split_payload, f, indent=2)\n",
    "    print(f\"Saved split manifest to {manifest_path} (seed={seed})\")\n",
    "    return split_payload\n",
    "\n",
    "\n",
    "split_manifest = build_split_manifest(DATA_ROOT, SPLIT_MANIFEST, seed=GLOBAL_SEED)\n",
    "class_names = split_manifest[\"class_names\"]\n",
    "num_classes = len(class_names)\n",
    "split_sizes = {k: len(v) for k, v in split_manifest[\"splits\"].items()}\n",
    "split_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d8ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_split_counts(manifest: Dict[str, List[str]]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for split_name, paths in manifest.items():\n",
    "        labels = [Path(p).parts[0] for p in paths]\n",
    "        counts = Counter(labels)\n",
    "        for cls in class_names:\n",
    "            rows.append({\"split\": split_name, \"class\": cls, \"images\": counts.get(cls, 0)})\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df.pivot(index=\"class\", columns=\"split\", values=\"images\").fillna(0).astype(int)\n",
    "\n",
    "\n",
    "split_summary = summarise_split_counts(split_manifest[\"splits\"])\n",
    "split_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56953a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_summary.plot(kind=\"bar\", figsize=(12, 5), title=\"RealWaste class distribution by split\")\n",
    "plt.ylabel(\"Number of images\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b0867a",
   "metadata": {},
   "source": [
    "### Dataset Utilities\n",
    "\n",
    "Define a lightweight dataset wrapper so each split can attach its own transform pipeline without duplicating underlying metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be472fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WasteDataset(Dataset):\n",
    "    def __init__(self, root: Path, samples: Iterable[str], class_to_idx: Dict[str, int], transform: Optional[Callable] = None):\n",
    "        self.root = root\n",
    "        self.samples = list(samples)\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.transform = transform\n",
    "        self.loader = datasets.folder.default_loader\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        rel_path = self.samples[idx]\n",
    "        label_name = Path(rel_path).parts[0]\n",
    "        target = self.class_to_idx[label_name]\n",
    "        path = self.root / rel_path\n",
    "        image = self.loader(path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class_to_idx = {name: idx for idx, name in enumerate(class_names)}\n",
    "train_samples = split_manifest[\"splits\"][\"train\"]\n",
    "val_samples = split_manifest[\"splits\"][\"val\"]\n",
    "test_samples = split_manifest[\"splits\"][\"test\"]\n",
    "\n",
    "len(train_samples), len(val_samples), len(test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a3d509",
   "metadata": {},
   "source": [
    "### Compute Dataset Normalisation Stats (for the custom CNN)\n",
    "\n",
    "To stabilise training we normalise inputs. The helper below caches channel-wise mean and standard deviation once computed, so repeat runs avoid recomputing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19177581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dataset_stats(samples: Iterable[str], cache_path: Path, image_size: int = 224) -> Tuple[List[float], List[float]]:\n",
    "    if cache_path.exists():\n",
    "        with cache_path.open() as f:\n",
    "            payload = json.load(f)\n",
    "        print(f\"Loaded cached dataset stats from {cache_path}\")\n",
    "        return payload[\"mean\"], payload[\"std\"]\n",
    "\n",
    "    transform = T.Compose([T.Resize((image_size, image_size)), T.ToTensor()])\n",
    "    dataset = WasteDataset(DATA_ROOT, samples, class_to_idx, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "    channel_sum = torch.zeros(3)\n",
    "    channel_squared_sum = torch.zeros(3)\n",
    "    total_pixels = 0\n",
    "\n",
    "    for images, _ in loader:\n",
    "        images = images.view(images.size(0), images.size(1), -1)\n",
    "        channel_sum += images.mean(dim=2).sum(dim=0)\n",
    "        channel_squared_sum += (images ** 2).mean(dim=2).sum(dim=0)\n",
    "        total_pixels += images.size(0)\n",
    "\n",
    "    mean = (channel_sum / total_pixels).tolist()\n",
    "    std = torch.sqrt((channel_squared_sum / total_pixels) - torch.tensor(mean) ** 2).tolist()\n",
    "\n",
    "    cache = {\"mean\": mean, \"std\": std, \"image_size\": image_size}\n",
    "    with cache_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cache, f, indent=2)\n",
    "    print(f\"Saved dataset stats to {cache_path}\")\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "CUSTOM_IMAGE_SIZE = 224\n",
    "custom_mean, custom_std = compute_dataset_stats(train_samples, STATS_CACHE, image_size=CUSTOM_IMAGE_SIZE)\n",
    "custom_mean, custom_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfb3622",
   "metadata": {},
   "source": [
    "### Transform Pipelines & DataLoaders\n",
    "\n",
    "Augmentations are conservative because the dataset already exhibits clutter and viewpoint variations. Validation/test splits only apply resizing and normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4680f401",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2 if DEVICE.type == \"cuda\" and os.name != \"nt\" else 0\n",
    "\n",
    "train_transform_custom = T.Compose(\n",
    "    [\n",
    "        T.Resize((CUSTOM_IMAGE_SIZE + 32, CUSTOM_IMAGE_SIZE + 32)),\n",
    "        T.RandomResizedCrop(CUSTOM_IMAGE_SIZE, scale=(0.8, 1.0)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=custom_mean, std=custom_std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "eval_transform_custom = T.Compose(\n",
    "    [\n",
    "        T.Resize((CUSTOM_IMAGE_SIZE, CUSTOM_IMAGE_SIZE)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=custom_mean, std=custom_std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def make_dataloader(sample_list: Iterable[str], transform: Callable, shuffle: bool) -> DataLoader:\n",
    "    dataset = WasteDataset(DATA_ROOT, sample_list, class_to_idx, transform=transform)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=(DEVICE.type == \"cuda\"),\n",
    "    )\n",
    "\n",
    "\n",
    "custom_loaders = {\n",
    "    \"train\": make_dataloader(train_samples, train_transform_custom, shuffle=True),\n",
    "    \"val\": make_dataloader(val_samples, eval_transform_custom, shuffle=False),\n",
    "    \"test\": make_dataloader(test_samples, eval_transform_custom, shuffle=False),\n",
    "}\n",
    "\n",
    "next(iter(custom_loaders[\"train\"]))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea1ad8f",
   "metadata": {},
   "source": [
    "### Peek at a Few Training Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85626c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dataloader: DataLoader, mean: List[float], std: List[float], title: str, max_images: int = 8) -> None:\n",
    "    images, targets = next(iter(dataloader))\n",
    "    inv_norm = T.Normalize(mean=[-m / s for m, s in zip(mean, std)], std=[1 / s for s in std])\n",
    "    images = [inv_norm(img).permute(1, 2, 0).clamp(0, 1).numpy() for img in images[:max_images]]\n",
    "    labels = [class_names[target] for target in targets[:max_images]]\n",
    "\n",
    "    cols = min(max_images, 4)\n",
    "    rows = math.ceil(len(images) / cols)\n",
    "    plt.figure(figsize=(3 * cols, 3 * rows))\n",
    "    for idx, (img, label) in enumerate(zip(images, labels), start=1):\n",
    "        ax = plt.subplot(rows, cols, idx)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(label)\n",
    "        ax.axis(\"off\")\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "show_batch(custom_loaders[\"train\"], custom_mean, custom_std, title=\"Augmented training samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc8de7",
   "metadata": {},
   "source": [
    "## 3. Custom CNN Architecture\n",
    "\n",
    "The network mirrors the assignment diagram: two convolution + pooling stages, a fully-connected projection, dropout, and a softmax output. Hyperparameters are selected to balance model capacity and overfitting risk on a ~4k-image dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea0b9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleWasteCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 3,\n",
    "        num_classes: int = num_classes,\n",
    "        conv_channels: Tuple[int, int] = (32, 64),\n",
    "        kernel_sizes: Tuple[int, int] = (3, 3),\n",
    "        fc_units: int = 256,\n",
    "        dropout: float = 0.3,\n",
    "        activation: Callable[[torch.Tensor], torch.Tensor] = F.relu,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, conv_channels[0], kernel_size=kernel_sizes[0], padding=1)\n",
    "        self.conv2 = nn.Conv2d(conv_channels[0], conv_channels[1], kernel_size=kernel_sizes[1], padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(conv_channels[0])\n",
    "        self.bn2 = nn.BatchNorm2d(conv_channels[1])\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self._feature_dim = conv_channels[1] * (CUSTOM_IMAGE_SIZE // 4) * (CUSTOM_IMAGE_SIZE // 4)\n",
    "        self.fc1 = nn.Linear(self._feature_dim, fc_units)\n",
    "        self.bn_fc = nn.BatchNorm1d(fc_units)\n",
    "        self.fc_out = nn.Linear(fc_units, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(self.activation(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.activation(self.bn2(self.conv2(x))))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(self.activation(self.bn_fc(self.fc1(x))))\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "custom_model = SimpleWasteCNN()\n",
    "custom_model.to(DEVICE)\n",
    "custom_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11353c1f",
   "metadata": {},
   "source": [
    "### Hyperparameter Justification (Questions 5 & 6)\n",
    "\n",
    "- **Activation**: ReLU offers fast convergence and avoids saturation; it keeps gradients stable in shallow CNNs without the compute overhead of GELU/Swish.\n",
    "- **Batch Normalisation**: BatchNorm layers follow each convolution and the dense projection to keep activations well-scaled, curbing vanishing/exploding gradients and accelerating convergence.\n",
    "- **Kernel sizes**: `3x3` kernels capture local texture features while keeping parameter count modest; padding preserves spatial resolution before pooling.\n",
    "- **Filter counts**: `(32, 64)` balances expressiveness and overfitting risk, roughly doubling channels per block as in LeNet-style architectures.\n",
    "- **Fully-connected width**: `256` units provide enough capacity to combine the learned spatial features without dominating the total parameter count.\n",
    "- **Dropout 0.3**: Regularises the dense layer; tuned via quick pilot runs to reduce overfitting while keeping validation accuracy stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffde101",
   "metadata": {},
   "source": [
    "## 4. Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47743a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HistoryRecord:\n",
    "    epoch: int\n",
    "    train_loss: float\n",
    "    train_acc: float\n",
    "    val_loss: float\n",
    "    val_acc: float\n",
    "    lr: float\n",
    "\n",
    "\n",
    "def accuracy_from_logits(logits: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "    preds = logits.argmax(dim=1)\n",
    "    return (preds == targets).float().mean().item()\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    dataloaders: Dict[str, DataLoader],\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,\n",
    "    num_epochs: int = 20,\n",
    "    device: torch.device = DEVICE,\n",
    "    use_amp: bool = True,\n",
    ") -> List[HistoryRecord]:\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp and device.type == \"cuda\")\n",
    "    history: List[HistoryRecord] = []\n",
    "    best_val_acc = 0.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_start = time.time()\n",
    "        metrics = {\"train\": {}, \"val\": {}}\n",
    "\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            model.train() if phase == \"train\" else model.eval()\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            total_samples = 0\n",
    "\n",
    "            for inputs, targets in dataloaders[phase]:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                batch_size = targets.size(0)\n",
    "\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    with torch.cuda.amp.autocast(enabled=use_amp and device.type == \"cuda\"):\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        optimizer.zero_grad()\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "\n",
    "                running_loss += loss.item() * batch_size\n",
    "                running_acc += accuracy_from_logits(outputs, targets) * batch_size\n",
    "                total_samples += batch_size\n",
    "\n",
    "            epoch_loss = running_loss / total_samples\n",
    "            epoch_acc = running_acc / total_samples\n",
    "            metrics[phase] = {\"loss\": epoch_loss, \"acc\": epoch_acc}\n",
    "\n",
    "        if scheduler and isinstance(scheduler, ReduceLROnPlateau):\n",
    "            scheduler.step(metrics[\"val\"][\"loss\"])\n",
    "        elif scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        history.append(\n",
    "            HistoryRecord(\n",
    "                epoch=epoch,\n",
    "                train_loss=metrics[\"train\"][\"loss\"],\n",
    "                train_acc=metrics[\"train\"][\"acc\"],\n",
    "                val_loss=metrics[\"val\"][\"loss\"],\n",
    "                val_acc=metrics[\"val\"][\"acc\"],\n",
    "                lr=current_lr,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if metrics[\"val\"][\"acc\"] > best_val_acc:\n",
    "            best_val_acc = metrics[\"val\"][\"acc\"]\n",
    "            best_state = model.state_dict()\n",
    "\n",
    "        elapsed = time.time() - epoch_start\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d}/{num_epochs} | \"\n",
    "            f\"train_loss={metrics['train']['loss']:.4f} val_loss={metrics['val']['loss']:.4f} | \"\n",
    "            f\"train_acc={metrics['train']['acc']:.3f} val_acc={metrics['val']['acc']:.3f} | \"\n",
    "            f\"lr={current_lr:.6f} | {elapsed:.1f}s\"\n",
    "        )\n",
    "\n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device = DEVICE,\n",
    ") -> Dict[str, object]:\n",
    "    model.eval()\n",
    "    targets_all: List[int] = []\n",
    "    preds_all: List[int] = []\n",
    "    probs_all: List[np.ndarray] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "            targets_all.extend(targets.numpy())\n",
    "            preds_all.extend(preds)\n",
    "            probs_all.extend(probs)\n",
    "\n",
    "    targets_np = np.array(targets_all)\n",
    "    preds_np = np.array(preds_all)\n",
    "    conf = confusion_matrix(targets_np, preds_np)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(targets_np, preds_np, average=\"macro\", zero_division=0)\n",
    "\n",
    "    report = classification_report(targets_np, preds_np, target_names=class_names, zero_division=0, output_dict=True)\n",
    "    accuracy = (preds_np == targets_np).mean()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_macro\": precision,\n",
    "        \"recall_macro\": recall,\n",
    "        \"f1_macro\": f1,\n",
    "        \"confusion_matrix\": conf,\n",
    "        \"report\": report,\n",
    "        \"targets\": targets_np,\n",
    "        \"predictions\": preds_np,\n",
    "        \"probabilities\": np.array(probs_all),\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_history(history: List[HistoryRecord], title: str) -> None:\n",
    "    epochs = [h.epoch for h in history]\n",
    "    train_loss = [h.train_loss for h in history]\n",
    "    val_loss = [h.val_loss for h in history]\n",
    "    train_acc = [h.train_acc for h in history]\n",
    "    val_acc = [h.val_acc for h in history]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    axes[0].plot(epochs, train_loss, label=\"train\")\n",
    "    axes[0].plot(epochs, val_loss, label=\"val\")\n",
    "    axes[0].set_title(f\"Loss - {title}\")\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].set_ylabel(\"Cross-entropy loss\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(epochs, train_acc, label=\"train\")\n",
    "    axes[1].plot(epochs, val_acc, label=\"val\")\n",
    "    axes[1].set_title(f\"Accuracy - {title}\")\n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].set_ylabel(\"Accuracy\")\n",
    "    axes[1].legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm: np.ndarray, labels: List[str], title: str) -> None:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d20987f",
   "metadata": {},
   "source": [
    "## 5. Optimiser & Learning-Rate Strategy (Questions 7-10)\n",
    "\n",
    "We evaluate three optimisers:\n",
    "\n",
    "1. **Adam** (baseline) - adaptive method for faster convergence.\n",
    "2. **SGD** - vanilla stochastic gradient descent.\n",
    "3. **SGD + Momentum** - adds momentum (0.9) to accelerate along valleys.\n",
    "\n",
    "Learning rates were selected via a short learning-rate range test (see optional cell below) and manual fine-tuning:\n",
    "\n",
    "- Adam: `1e-4` with `weight_decay=1e-4`.\n",
    "- SGD: `0.01` with `weight_decay=5e-4`.\n",
    "- SGD + Momentum: `0.01`, `momentum=0.9`, `weight_decay=5e-4`.\n",
    "\n",
    "All runs train for 20 epochs with `ReduceLROnPlateau` on validation loss and Automatic Mixed Precision enabled when CUDA is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d6d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(optimizer_name: str, parameters) -> torch.optim.Optimizer:\n",
    "    if optimizer_name == \"adam\":\n",
    "        return torch.optim.Adam(parameters, lr=1e-4, weight_decay=1e-4)\n",
    "    if optimizer_name == \"sgd\":\n",
    "        return torch.optim.SGD(parameters, lr=0.01, momentum=0.0, weight_decay=5e-4)\n",
    "    if optimizer_name == \"sgd_momentum\":\n",
    "        return torch.optim.SGD(parameters, lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "    raise ValueError(f\"Unknown optimizer: {optimizer_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12058150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_custom_cnn_experiment(optimizer_key: str, num_epochs: int = 20) -> Dict[str, object]:\n",
    "    set_seed(GLOBAL_SEED)\n",
    "    model = SimpleWasteCNN().to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = make_optimizer(optimizer_key, model.parameters())\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "    history = train_model(\n",
    "        model,\n",
    "        dataloaders={\"train\": custom_loaders[\"train\"], \"val\": custom_loaders[\"val\"]},\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        scheduler=scheduler,\n",
    "        num_epochs=num_epochs,\n",
    "        use_amp=True,\n",
    "    )\n",
    "\n",
    "    metrics = evaluate_model(model, custom_loaders[\"test\"])\n",
    "\n",
    "    weights_path = ARTIFACT_DIR / f\"simple_cnn_{optimizer_key}.pt\"\n",
    "    torch.save({\"model_state_dict\": model.state_dict(), \"class_names\": class_names}, weights_path)\n",
    "\n",
    "    return {\n",
    "        \"optimizer\": optimizer_key,\n",
    "        \"history\": history,\n",
    "        \"metrics\": metrics,\n",
    "        \"weights_path\": weights_path,\n",
    "        \"model\": model,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ec1e55",
   "metadata": {},
   "source": [
    "Run the three experiments separately so GPU memory is freed between runs (execute the cell below three times changing the key, or wrap it in a loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb00694",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_results = run_custom_cnn_experiment(\"adam\")\n",
    "plot_history(adam_results[\"history\"], title=\"Simple CNN - Adam\")\n",
    "plot_confusion_matrix(adam_results[\"metrics\"][\"confusion_matrix\"], class_names, \"Simple CNN - Adam (Test)\")\n",
    "\n",
    "print(\"Adam classification report:\")\n",
    "print(adam_results[\"metrics\"][\"report\"])\n",
    "\n",
    "adam_metrics_table = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"model\": \"Simple CNN (Adam)\",\n",
    "            \"test_accuracy\": adam_results[\"metrics\"][\"accuracy\"],\n",
    "            \"precision_macro\": adam_results[\"metrics\"][\"precision_macro\"],\n",
    "            \"recall_macro\": adam_results[\"metrics\"][\"recall_macro\"],\n",
    "            \"f1_macro\": adam_results[\"metrics\"][\"f1_macro\"],\n",
    "        }\n",
    "    ]\n",
    ").set_index(\"model\")\n",
    "adam_metrics_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b80de7",
   "metadata": {},
   "source": [
    "### Optional: Learning-Rate Range Test\n",
    "\n",
    "Use this helper to visualise loss vs. learning rate for a quick heuristic before finalising the schedule. Run on a training subset to keep it lightweight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0973eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_range_test(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    lr_start: float = 1e-6,\n",
    "    lr_end: float = 1.0,\n",
    "    num_iters: int = 100,\n",
    ") -> pd.DataFrame:\n",
    "    total_iters = min(num_iters, len(dataloader))\n",
    "    lrs = torch.logspace(math.log10(lr_start), math.log10(lr_end), steps=total_iters)\n",
    "    losses = []\n",
    "\n",
    "    for lr, (inputs, targets) in zip(lrs, dataloader):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr.item()\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return pd.DataFrame({\"lr\": lrs.numpy(), \"loss\": losses})\n",
    "\n",
    "\n",
    "# Usage:\n",
    "# subset_loader = DataLoader(WasteDataset(DATA_ROOT, train_samples[:512], class_to_idx, transform=train_transform_custom), batch_size=64)\n",
    "# model = SimpleWasteCNN().to(DEVICE)\n",
    "# df = lr_range_test(model, subset_loader, torch.optim.SGD(model.parameters(), lr=1e-3), nn.CrossEntropyLoss())\n",
    "# df.plot(x=\"lr\", y=\"loss\", logx=True, title=\"LR range test (SGD)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e7abe",
   "metadata": {},
   "source": [
    "### Question 11 - Momentum Analysis\n",
    "\n",
    "Once `sgd` and `sgd_momentum` runs complete, reuse their histories to discuss how momentum affected convergence (e.g., faster loss decay, improved validation accuracy). Optionally, probe an intermediate momentum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643a9040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_sweep(momentum_values: Iterable[float], num_epochs: int = 10) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for momentum in momentum_values:\n",
    "        key = f\"sgd_m{momentum:.2f}\"\n",
    "        set_seed(GLOBAL_SEED)\n",
    "        model = SimpleWasteCNN().to(DEVICE)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=momentum, weight_decay=5e-4)\n",
    "        history = train_model(\n",
    "            model,\n",
    "            dataloaders={\"train\": custom_loaders[\"train\"], \"val\": custom_loaders[\"val\"]},\n",
    "            optimizer=optimizer,\n",
    "            criterion=nn.CrossEntropyLoss(),\n",
    "            scheduler=ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=3),\n",
    "            num_epochs=num_epochs,\n",
    "            use_amp=True,\n",
    "        )\n",
    "        metrics = evaluate_model(model, custom_loaders[\"val\"])\n",
    "        rows.append(\n",
    "            {\n",
    "                \"momentum\": momentum,\n",
    "                \"val_accuracy\": metrics[\"accuracy\"],\n",
    "                \"final_train_loss\": history[-1].train_loss,\n",
    "                \"final_val_loss\": history[-1].val_loss,\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Example: momentum_results = momentum_sweep([0.0, 0.5, 0.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae37846",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation (Question 12)\n",
    "\n",
    "After training each optimiser, populate the table below with accuracy, precision, recall, and F1 scores, and capture the confusion matrix plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc1f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_metrics_table(results: List[Dict[str, object]]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for res in results:\n",
    "        metrics = res[\"metrics\"]\n",
    "        rows.append(\n",
    "            {\n",
    "                \"model\": f\"Simple CNN ({res['optimizer']})\",\n",
    "                \"test_accuracy\": metrics[\"accuracy\"],\n",
    "                \"precision_macro\": metrics[\"precision_macro\"],\n",
    "                \"recall_macro\": metrics[\"recall_macro\"],\n",
    "                \"f1_macro\": metrics[\"f1_macro\"],\n",
    "            }\n",
    "        )\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df.sort_values(by=\"test_accuracy\", ascending=False)\n",
    "\n",
    "\n",
    "# Example usage once the runs complete:\n",
    "# custom_results = [adam_results, sgd_results, sgd_m_results]\n",
    "# build_metrics_table(custom_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96e6671",
   "metadata": {},
   "source": [
    "## 7. Transfer Learning (Questions 13-18)\n",
    "\n",
    "Fine-tune two pre-trained models - here we use **ResNet-18** and **DenseNet-121** - on the same data split. They use ImageNet mean/std for normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c90572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pretrained_transforms(mean: List[float], std: List[float], image_size: int = 224) -> Tuple[Callable, Callable]:\n",
    "    train_transform = T.Compose(\n",
    "        [\n",
    "            T.Resize((image_size + 32, image_size + 32)),\n",
    "            T.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=mean, std=std),\n",
    "        ]\n",
    "    )\n",
    "    eval_transform = T.Compose(\n",
    "        [\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=mean, std=std),\n",
    "        ]\n",
    "    )\n",
    "    return train_transform, eval_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e7418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pretrained_model(name: str, num_classes: int = num_classes) -> Tuple[nn.Module, Callable, Callable]:\n",
    "    if name == \"resnet18\":\n",
    "        weights = models.ResNet18_Weights.DEFAULT\n",
    "        base_model = models.resnet18(weights=weights)\n",
    "        base_model.fc = nn.Linear(base_model.fc.in_features, num_classes)\n",
    "    elif name == \"densenet121\":\n",
    "        weights = models.DenseNet121_Weights.DEFAULT\n",
    "        base_model = models.densenet121(weights=weights)\n",
    "        base_model.classifier = nn.Linear(base_model.classifier.in_features, num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {name}\")\n",
    "\n",
    "    train_transform, eval_transform = build_pretrained_transforms(weights.meta[\"mean\"], weights.meta[\"std\"], image_size=224)\n",
    "    return base_model, train_transform, eval_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0118e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(\n",
    "    model_name: str,\n",
    "    num_epochs: int = 20,\n",
    "    base_lr: float = 1e-4,\n",
    "    weight_decay: float = 1e-4,\n",
    ") -> Dict[str, object]:\n",
    "    set_seed(GLOBAL_SEED)\n",
    "    model, train_transform, eval_transform = build_pretrained_model(model_name)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    loaders = {\n",
    "        \"train\": make_dataloader(train_samples, train_transform, shuffle=True),\n",
    "        \"val\": make_dataloader(val_samples, eval_transform, shuffle=False),\n",
    "        \"test\": make_dataloader(test_samples, eval_transform, shuffle=False),\n",
    "    }\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=3, verbose=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    history = train_model(\n",
    "        model,\n",
    "        dataloaders={\"train\": loaders[\"train\"], \"val\": loaders[\"val\"]},\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        scheduler=scheduler,\n",
    "        num_epochs=num_epochs,\n",
    "        use_amp=True,\n",
    "    )\n",
    "\n",
    "    metrics = evaluate_model(model, loaders[\"test\"])\n",
    "    weights_path = ARTIFACT_DIR / f\"{model_name}_finetuned.pt\"\n",
    "    torch.save({\"model_state_dict\": model.state_dict(), \"class_names\": class_names}, weights_path)\n",
    "\n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"history\": history,\n",
    "        \"metrics\": metrics,\n",
    "        \"weights_path\": weights_path,\n",
    "        \"model\": model,\n",
    "        \"loaders\": loaders,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f05cec4",
   "metadata": {},
   "source": [
    "Run each fine-tuning experiment separately and record the outputs below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4407f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "# resnet_results = fine_tune_model(\"resnet18\")\n",
    "# plot_history(resnet_results[\"history\"], title=\"ResNet-18 Fine-tuning\")\n",
    "# plot_confusion_matrix(resnet_results[\"metrics\"][\"confusion_matrix\"], class_names, \"ResNet-18 - Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e7d41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "# densenet_results = fine_tune_model(\"densenet121\")\n",
    "# plot_history(densenet_results[\"history\"], title=\"DenseNet-121 Fine-tuning\")\n",
    "# plot_confusion_matrix(densenet_results[\"metrics\"][\"confusion_matrix\"], class_names, \"DenseNet-121 - Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0909817",
   "metadata": {},
   "source": [
    "### Consolidated Transfer-Learning Metrics\n",
    "\n",
    "After running both experiments, assemble them into a comparison table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3324e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_transfer_results(results: List[Dict[str, object]]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for res in results:\n",
    "        metrics = res[\"metrics\"]\n",
    "        rows.append(\n",
    "            {\n",
    "                \"model\": res[\"model_name\"],\n",
    "                \"test_accuracy\": metrics[\"accuracy\"],\n",
    "                \"precision_macro\": metrics[\"precision_macro\"],\n",
    "                \"recall_macro\": metrics[\"recall_macro\"],\n",
    "                \"f1_macro\": metrics[\"f1_macro\"],\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows).sort_values(by=\"test_accuracy\", ascending=False)\n",
    "\n",
    "\n",
    "# Example: summarise_transfer_results([resnet_results, densenet_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f336d0",
   "metadata": {},
   "source": [
    "## 8. Custom vs. Pre-trained - Comparative Analysis (Questions 18 & 19)\n",
    "\n",
    "Once all experiments finish, use the cell below to merge metrics and drive the written discussion on:\n",
    "\n",
    "- Accuracy/recall trade-offs.\n",
    "- Training time vs. performance.\n",
    "- Model size and inference cost.\n",
    "- Data efficiency and overfitting behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf49ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_results(custom_runs: List[Dict[str, object]], transfer_runs: List[Dict[str, object]]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for res in custom_runs:\n",
    "        metrics = res[\"metrics\"]\n",
    "        rows.append(\n",
    "            {\n",
    "                \"model\": f\"Custom CNN ({res['optimizer']})\",\n",
    "                \"category\": \"custom\",\n",
    "                \"accuracy\": metrics[\"accuracy\"],\n",
    "                \"precision_macro\": metrics[\"precision_macro\"],\n",
    "                \"recall_macro\": metrics[\"recall_macro\"],\n",
    "                \"f1_macro\": metrics[\"f1_macro\"],\n",
    "            }\n",
    "        )\n",
    "    for res in transfer_runs:\n",
    "        metrics = res[\"metrics\"]\n",
    "        rows.append(\n",
    "            {\n",
    "                \"model\": f\"Pretrained {res['model_name']}\",\n",
    "                \"category\": \"pretrained\",\n",
    "                \"accuracy\": metrics[\"accuracy\"],\n",
    "                \"precision_macro\": metrics[\"precision_macro\"],\n",
    "                \"recall_macro\": metrics[\"recall_macro\"],\n",
    "                \"f1_macro\": metrics[\"f1_macro\"],\n",
    "            }\n",
    "        )\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df.sort_values(by=\"accuracy\", ascending=False)\n",
    "\n",
    "\n",
    "# Example:\n",
    "# comparison_df = compare_all_results(custom_results, [resnet_results, densenet_results])\n",
    "# comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8d0854",
   "metadata": {},
   "source": [
    "### Narrative Prompts\n",
    "\n",
    "- _Optimizer comparison (Q10 & Q11)_: Summarise which optimiser performed best, how momentum changed convergence, and reference plots/tables.\n",
    "- _Evaluation results (Q12)_: Insert the confusion matrix figure and discuss class-wise strengths/weaknesses.\n",
    "- _Transfer learning (Q13-Q18)_: Highlight performance gains or trade-offs when switching to pre-trained models.\n",
    "- _Custom vs. Pre-trained (Q19)_: Reflect on resource requirements (training time, GPU memory), interpretability, and deployment considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee1c847",
   "metadata": {},
   "source": [
    "## 9. Export Artefacts for Submission\n",
    "\n",
    "Store trained weights and optionally export plots/tables for the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6192f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics_to_json(results: List[Dict[str, object]], path: Path) -> None:\n",
    "    serialisable = []\n",
    "    for res in results:\n",
    "        metrics = res[\"metrics\"].copy()\n",
    "        metrics[\"confusion_matrix\"] = metrics[\"confusion_matrix\"].tolist()\n",
    "        serialisable.append({\"label\": res.get(\"optimizer\", res.get(\"model_name\")), \"metrics\": metrics})\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(serialisable, f, indent=2)\n",
    "\n",
    "\n",
    "# Example:\n",
    "# save_metrics_to_json(custom_results, ARTIFACT_DIR / \"custom_cnn_metrics.json\")\n",
    "# save_metrics_to_json([resnet_results, densenet_results], ARTIFACT_DIR / \"transfer_metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e543b7df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Execute the notebook top-to-bottom, filling in results tables/plots.\n",
    "2. Capture figures for the report (training curves, confusion matrices).\n",
    "3. Summarise findings in the report template, referencing the artefacts saved under `artifacts/`.\n",
    "\n",
    "Good luck with the submission!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
